{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "ByteTensor = torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return Variable(LongTensor([word2index[word]]) if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:500]\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(flatten(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "        \n",
    "index2word={v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 5\n",
    "windows =  flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1)) for c in corpus])\n",
    "\n",
    "window_data = []\n",
    "\n",
    "for window in windows:\n",
    "    for i in range(WINDOW_SIZE * 2 + 1):\n",
    "        if i == WINDOW_SIZE or window[i] == '<DUMMY>': \n",
    "            continue\n",
    "        window_data.append((window[WINDOW_SIZE], window[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 'moby'),\n",
       " ('[', 'dick'),\n",
       " ('[', 'by'),\n",
       " ('[', 'herman'),\n",
       " ('[', 'melville'),\n",
       " ('moby', '['),\n",
       " ('moby', 'dick'),\n",
       " ('moby', 'by'),\n",
       " ('moby', 'herman'),\n",
       " ('moby', 'melville')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/54082903-d3731700-435f-11e9-9588-49a6397d0ccd.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j):\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "        ## 동시 출현 빈도\n",
    "    except:\n",
    "        x_ij = 1\n",
    "        \n",
    "    x_max = 100 #100 # fixed in paper\n",
    "    alpha = 0.75\n",
    "    \n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max)**alpha\n",
    "    else:\n",
    "        result = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Co-occurence Matirx X\n",
    "Because of model complexity, It is important to determine whether a tighter bound can be placed on the number of nonzero elements o f X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_i = Counter(flatten(corpus)) # X_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik_window_5 = Counter(window_data) # Co-occurece in window size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', ','), 342),\n",
       " ((',', 'the'), 280),\n",
       " (('the', ','), 280),\n",
       " (('the', 'of'), 248),\n",
       " (('of', 'the'), 248),\n",
       " ((',', 'and'), 229),\n",
       " (('and', ','), 229),\n",
       " (('the', 'the'), 220),\n",
       " ((',', 'a'), 167),\n",
       " (('a', ','), 167)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ik_window_5.most_common(10) # 가장 많이 등장하는 조합은 ,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik = {}\n",
    "weighting_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bigram in combinations_with_replacement(vocab, 2):\n",
    "    # ('fields', 'hunks')와 같은 조합을 배출한다. \n",
    "    if X_ik_window_5.get(bigram) is not None: # nonzero elements\n",
    "        co_occer = X_ik_window_5[bigram]\n",
    "        X_ik[bigram] = co_occer + 1 # log(Xik) -> log(Xik+1) to prevent divergence\n",
    "        X_ik[(bigram[1],bigram[0])] = co_occer+1\n",
    "    # ('fields','hunks'), ('hunks','fields') 둘다 딕셔너리에 넣어준다.\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1])\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('late', 'bedford')\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test = random.choice(window_data)\n",
    "print(test)\n",
    "try:\n",
    "    print(X_ik[(test[0], test[1])] == X_ik[(test[1], test[0])])\n",
    "except:\n",
    "    1\n",
    "    \n",
    "# ('fields','hunks'), ('hunks','fields') 나 출현빈도는 같을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_p = [] # center vec\n",
    "v_p = [] # context vec\n",
    "co_p = [] # log(x_ij)\n",
    "weight_p = [] # f(x_ij)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 'moby'), ('[', 'dick'), ('[', 'by'), ('[', 'herman'), ('[', 'melville')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in window_data: \n",
    "    u_p.append(prepare_word(pair[0], word2index).view(1, -1)) # ('[','moby')에서 '['\n",
    "    v_p.append(prepare_word(pair[1], word2index).view(1, -1)) # ('[','moby')에서 'moby'\n",
    "    \n",
    "    try:\n",
    "        cooc = X_ik[pair]\n",
    "    except: # 단어조합이 없을 경우 1\n",
    "        cooc = 1\n",
    "\n",
    "    co_p.append(torch.log(Variable(FloatTensor([cooc]))).view(1, -1))\n",
    "    weight_p.append(Variable(FloatTensor([weighting_dic[pair]])).view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[2519]]), tensor([[2475]]), tensor([[0.6931]]), tensor([[0.0532]]))\n"
     ]
    }
   ],
   "source": [
    "train_data = list(zip(u_p, v_p, co_p, weight_p))\n",
    "del u_p # 용량 차지하니까 없에준다\n",
    "del v_p\n",
    "del co_p\n",
    "del weight_p\n",
    "print(train_data[0]) # tuple (center vec i, context vec j log(x_ij), weight f(w_ij))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "- 목적함수를 이해해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목적함수를 이해해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/54083506-a3c80d00-4367-11e9-847e-1be9bf8f29b5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/54083507-a9bdee00-4367-11e9-8d44-1b0f26995662.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(Glove, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim) # center embedding matrix\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim) # out embedding matrix\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "        initrange = (2.0 / (vocab_size + projection_dim)) ** 0.5 # Xavier init\n",
    "        self.embedding_v.weight.data.uniform_(-initrange, initrange) # init\n",
    "        self.embedding_u.weight.data.uniform_(-initrange, initrange) # init\n",
    "        self.v_bias.weight.data.uniform_(-initrange, initrange) # init\n",
    "        self.u_bias.weight.data.uniform_(-initrange, initrange) # init\n",
    "        \n",
    "    def forward(self, center_words, target_words, coocs, weights):\n",
    "        center_embeds = self.embedding_v(center_words) # B X 1 X D\n",
    "        target_embeds = self.embedding_u(target_words) # B X 1 X D\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1) # B X 1\n",
    "        target_bias = self.u_bias(target_words).squeeze(1) # B X 1\n",
    "        \n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1,2)).squeeze(2) # BX1\n",
    "        \n",
    "        loss = weights * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        v_embeds = self.embedding_v(inputs) # B X 1 X D\n",
    "        u_embeds = self.embedding_u(inputs) # B X 1 X D\n",
    "        \n",
    "        return v_embeds + u_embeds # final embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 50\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "model = Glove(len(word2index), EMBEDDING_SIZE)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 225.57\n",
      "Epoch : 10, mean_loss : 2.60\n",
      "Epoch : 20, mean_loss : 0.55\n",
      "Epoch : 30, mean_loss : 0.12\n",
      "Epoch : 40, mean_loss : 0.04\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        \n",
    "        inputs, targets, coocs, weights = zip(*batch)\n",
    "        \n",
    "        inputs, targets, coocs, weights = zip(*batch)\n",
    "        \n",
    "        inputs = torch.cat(inputs) # B X 1\n",
    "        targets = torch.cat(targets) # B X 1\n",
    "        coocs = torch.cat(coocs)\n",
    "        weights = torch.cat(weights)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        loss = model(inputs, targets, coocs, weights)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.data)\n",
    "        \n",
    "    if epoch% 10 ==0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch, np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(target, vocab):\n",
    "    target_V = model.prediction(prepare_word(target, word2index))\n",
    "    similarities = []\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == target: \n",
    "            continue\n",
    "        \n",
    "        vector = model.prediction(prepare_word(list(vocab)[i], word2index))\n",
    "        \n",
    "        cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0] \n",
    "        similarities.append([vocab[i], cosine_sim])\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiment'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = random.choice(list(vocab))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['caravan', 0.8281784057617188],\n",
       " ['needles', 0.7806263566017151],\n",
       " ['ahoy', 0.7694544792175293],\n",
       " [',', 0.7557148337364197],\n",
       " ['cheever', 0.7556502819061279],\n",
       " ['ah', 0.7461116313934326],\n",
       " ['crucifix', 0.7429143786430359],\n",
       " ['hval', 0.7399888634681702],\n",
       " ['maxim', 0.7346227169036865],\n",
       " ['happen', 0.734491229057312]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(test, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
