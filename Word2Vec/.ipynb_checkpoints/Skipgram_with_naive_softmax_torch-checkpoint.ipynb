{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Skipgram with naiive softmax\n",
    "Regference 김성동님 [DeepNLP-models-Pytorch](https://github.com/DSKSD/DeepNLP-models-Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n",
      "3.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "ByteTensor = torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "        \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "\n",
    "# seq에 해당되는 key값의 value를 리스트에 모으는거\n",
    "\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return Variable(LongTensor([word2index[word]]) if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]]))\n",
    "\n",
    "## prepare_word : 해당단어의 wordindex LongTensor로 가져옴\n",
    "## sequence와의 차이점은 loop가아니라는거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and Preprocessing\n",
    "\n",
    "#### Load corpus : Gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:100] #  sampling sentences for test\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']'],\n",
       " ['etymology', '.'],\n",
       " ['(',\n",
       "  'supplied',\n",
       "  'by',\n",
       "  'a',\n",
       "  'late',\n",
       "  'consumptive',\n",
       "  'usher',\n",
       "  'to',\n",
       "  'a',\n",
       "  'grammar',\n",
       "  'school',\n",
       "  ')'],\n",
       " ['the',\n",
       "  'pale',\n",
       "  'usher',\n",
       "  '--',\n",
       "  'threadbare',\n",
       "  'in',\n",
       "  'coat',\n",
       "  ',',\n",
       "  'heart',\n",
       "  ',',\n",
       "  'body',\n",
       "  ',',\n",
       "  'and',\n",
       "  'brain',\n",
       "  ';',\n",
       "  'i',\n",
       "  'see',\n",
       "  'him',\n",
       "  'now',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'was',\n",
       "  'ever',\n",
       "  'dusting',\n",
       "  'his',\n",
       "  'old',\n",
       "  'lexicons',\n",
       "  'and',\n",
       "  'grammars',\n",
       "  ',',\n",
       "  'with',\n",
       "  'a',\n",
       "  'queer',\n",
       "  'handkerchief',\n",
       "  ',',\n",
       "  'mockingly',\n",
       "  'embellished',\n",
       "  'with',\n",
       "  'all',\n",
       "  'the',\n",
       "  'gay',\n",
       "  'flags',\n",
       "  'of',\n",
       "  'all',\n",
       "  'the',\n",
       "  'known',\n",
       "  'nations',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  '.']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Stopwords from unigram distribution's tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(flatten(corpus))\n",
    "border = int(len(word_count) * 0.01)  ## 592개의 단어 있어서 상위 하위 5개 제외할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = word_count.most_common()[:border]+list(reversed(word_count.most_common()))[:border]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords =[s[0] for s in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', 'the', 'of', 'and', 'man', 'artificial', 'civitas', '--(', 'state']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(flatten(corpus)) - set(stopwords))\n",
    "vocab.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592 583\n"
     ]
    }
   ],
   "source": [
    "print(len(set(flatten(corpus))), len(vocab))\n",
    "# 원래 592개에서 10개 불용어 빼고 UNK하나 추가해서 583개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {'<UNK>':0}\n",
    "\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "        \n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "## {0:'<UNK>',1:'however',2:'foul'...}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare train data\n",
    "window data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 3\n",
    "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1)) for c in corpus])\n",
    "# 각각의 corpus에 대해서 window size가 주어지면 첫번째 단어는 왼쪽에 단어가 없어서 Window_size만큼 Dummy를 주는 작업. 뒤에 코딩 편하도록\n",
    "\n",
    "# [('<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by'),\n",
    "#  ('<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman'),\n",
    "#  ('<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville'),\n",
    "#  ('[', 'moby', 'dick', 'by', 'herman', 'melville', '1851'),\n",
    "#  ('moby', 'dick', 'by', 'herman', 'melville', '1851', ']'),\n",
    "#  ('dick', 'by', 'herman', 'melville', '1851', ']', '<DUMMY>'),\n",
    "#  ('by', 'herman', 'melville', '1851', ']', '<DUMMY>', '<DUMMY>'),\n",
    "#  ('herman', 'melville', '1851', ']', '<DUMMY>', '<DUMMY>', '<DUMMY>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1463"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(windows) ## 1463개의 training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'moby'), ('[', 'dick'), ('[', 'by'), ('moby', '['), ('moby', 'dick'), ('moby', 'by')]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "for window in windows:                                 # Skip gram\n",
    "    for i in range(WINDOW_SIZE * 2 + 1):               # centerword + 2 * WINDOW_SIZE = 7\n",
    "        if i == WINDOW_SIZE or window[i] == '<DUMMY>': # i  = WINDOW_SIZE는 centerword라는 뜻\n",
    "            continue                                   # i번째 단어가 centerword이거나 'DUMMY'이면 넘어가라!\n",
    "        train_data.append((window[WINDOW_SIZE], window[i]))\n",
    "    \n",
    "print(train_data[:WINDOW_SIZE*2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p = []\n",
    "y_p = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[', 'moby')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0] # input이 '[' label 이 'moby'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in train_data:\n",
    "    X_p.append(prepare_word(tr[0], word2index).view(1,-1)) # -1은 미지수 n 차원일부로 늘리는거 (  [] 를 하나 더 씌운다.)\n",
    "    y_p.append(prepare_word(tr[1], word2index).view(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[210]])\n",
      "tensor([[360]])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_p[0])\n",
    "print(y_p[0])\n",
    "print(X_p[0].shape) # 차원하나 더늘려서 (1,1)됨  \n",
    "                    # ex) [[64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7606"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = list(zip(X_p, y_p))\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6283, -0.0893,  0.7993],\n",
       "         [ 0.3314, -0.0602,  1.7140],\n",
       "         [ 0.1613,  0.6638,  0.6927],\n",
       "         [ 0.2260,  1.2989, -0.6374]],\n",
       "\n",
       "        [[ 0.1613,  0.6638,  0.6927],\n",
       "         [ 1.1045, -0.1108,  0.5328],\n",
       "         [ 0.3314, -0.0602,  1.7140],\n",
       "         [-0.0591, -0.1103,  0.9381]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an Embedding module containing 10 tensors of size 3\n",
    "embedding = nn.Embedding(10,3)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim)\n",
    "        \n",
    "        self.embedding_v.weight.data.uniform_(-1,1)  # init\n",
    "        self.embedding_u.weight.data.uniform_(0,0)   # init\n",
    "        # self.out = nn.Linear(projection_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, center_words, target_words, outer_words):\n",
    "        # outer_words가 BXV 즉 [1,2,3,4,...582,0]\n",
    "        # 이걸 넣으면 BXV에 디멘션 추가 BXVXD 가 됨\n",
    "        center_embeds = self.embedding_v(center_words) # B X 1 X D  center_word의 인덱스에 해당하는 하나의 행을 가져오는듯\n",
    "        target_embeds = self.embedding_u(target_words) # B X 1 X D\n",
    "        outer_embeds = self.embedding_u(outer_words)   # B X V X D \n",
    "        \n",
    "        scores = target_embeds.bmm(center_embeds.transpose(1,2)).squeeze(2)    # BX1XD * BXDX1 => BX1X1 => BX1   3번째 index가 1이면 없에줌\n",
    "        norm_scores = outer_embeds.bmm(center_embeds.transpose(1,2)).squeeze(2) # BXVXD * BXDX1 => BXVXD * BXDX1=> BXV\n",
    "                                                                                # 여기 계산이 굉장히 비효울적. Vectorsize가 몇십만개 일수도있는데 ...\n",
    "        \n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/ torch.sum(torch.exp(norm_scores),1).unsqueeze(1))) \n",
    "        # log-softmax 2번째 index에 1을 넣어줌\n",
    "        # torch.sum(torch.exp(num_scores),1) 각 배치별로 Vocab개수만큼의 exp값이 있으면\n",
    "        return nll # negative log likelihood\n",
    "    \n",
    "    def prediction(self,inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        \n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/54069443-6b5afd00-429b-11e9-97bf-ff5e525d93d7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/54069351-ab6db000-429a-11e9-8e99-14764ac89a83.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 30\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "model = Skipgram(len(word2index), EMBEDDING_SIZE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = prepare_sequence(list(vocab), word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'great': 1,\n",
       " 'somehow': 2,\n",
       " 'them': 3,\n",
       " 'grow': 4,\n",
       " 'painstaking': 5,\n",
       " 'has': 6,\n",
       " 'men': 7,\n",
       " 'fixed': 8,\n",
       " 'pikes': 9,\n",
       " 'splintered': 10,\n",
       " 'foam': 11,\n",
       " 'handkerchief': 12,\n",
       " 'deep': 13,\n",
       " 'solely': 14,\n",
       " 't': 15,\n",
       " 'burrower': 16,\n",
       " 'exceeding': 17,\n",
       " 'paunch': 18,\n",
       " 'pale': 19,\n",
       " 'dreadful': 20,\n",
       " 'letter': 21,\n",
       " 'rosy': 22,\n",
       " 'down': 23,\n",
       " 'many': 24,\n",
       " 'pliny': 25,\n",
       " 'created': 26,\n",
       " 'slay': 27,\n",
       " 'history': 28,\n",
       " 'hie': 29,\n",
       " 'brought': 30,\n",
       " 'ignorance': 31,\n",
       " 'received': 32,\n",
       " 'please': 33,\n",
       " 'hampton': 34,\n",
       " 'loves': 35,\n",
       " 'pekee': 36,\n",
       " 'clear': 37,\n",
       " 'morals': 38,\n",
       " 'prepared': 39,\n",
       " 'for': 40,\n",
       " 'him': 41,\n",
       " 'sw': 42,\n",
       " 'sung': 43,\n",
       " 'including': 44,\n",
       " 'maine': 45,\n",
       " 'from': 46,\n",
       " 'when': 47,\n",
       " 'security': 48,\n",
       " 'spermacetti': 49,\n",
       " 'monster': 50,\n",
       " 'taken': 51,\n",
       " 'besides': 52,\n",
       " 'whatever': 53,\n",
       " 'whether': 54,\n",
       " 'boiling': 55,\n",
       " 'bred': 56,\n",
       " '890': 57,\n",
       " 'higgledy': 58,\n",
       " 'very': 59,\n",
       " 'danish': 60,\n",
       " 'case': 61,\n",
       " 'sub': 62,\n",
       " 'with': 63,\n",
       " 'mockingly': 64,\n",
       " 'swallow': 65,\n",
       " 'verbal': 66,\n",
       " 'mouth': 67,\n",
       " 'erromangoan': 68,\n",
       " 'quantity': 69,\n",
       " 'gulf': 70,\n",
       " 'sixty': 71,\n",
       " 'now': 72,\n",
       " 'convivial': 73,\n",
       " 'dusting': 74,\n",
       " 'shall': 75,\n",
       " 'liver': 76,\n",
       " 'random': 77,\n",
       " 'enter': 78,\n",
       " 'melville': 79,\n",
       " 'appears': 80,\n",
       " 'bruise': 81,\n",
       " 'battle': 82,\n",
       " 'are': 83,\n",
       " 'incredible': 84,\n",
       " 'gospel': 85,\n",
       " 'upon': 86,\n",
       " 's': 87,\n",
       " 'name': 88,\n",
       " 'threadbare': 89,\n",
       " 'bottomless': 90,\n",
       " 'jav': 91,\n",
       " 'dick': 92,\n",
       " 'our': 93,\n",
       " 'icelandic': 94,\n",
       " 'acres': 95,\n",
       " 'raising': 96,\n",
       " 'v': 97,\n",
       " 'henry': 98,\n",
       " 'at': 99,\n",
       " 'bacon': 100,\n",
       " 'body': 101,\n",
       " 'day': 102,\n",
       " 'patient': 103,\n",
       " '--': 104,\n",
       " 'foul': 105,\n",
       " '\"': 106,\n",
       " 'forty': 107,\n",
       " 'bulk': 108,\n",
       " 'vaticans': 109,\n",
       " 'teeth': 110,\n",
       " 'much': 111,\n",
       " 'alone': 112,\n",
       " 'learned': 113,\n",
       " 'in': 114,\n",
       " 'grammar': 115,\n",
       " 'hast': 116,\n",
       " 'thou': 117,\n",
       " 'size': 118,\n",
       " 'arpens': 119,\n",
       " 'even': 120,\n",
       " 'called': 121,\n",
       " 'cetology': 122,\n",
       " 'also': 123,\n",
       " ').': 124,\n",
       " 'poor': 125,\n",
       " 'sometimes': 126,\n",
       " 'more': 127,\n",
       " 'holland': 128,\n",
       " 'thirty': 129,\n",
       " 'dust': 130,\n",
       " 'world': 131,\n",
       " 'hearts': 132,\n",
       " 'prophet': 133,\n",
       " 'heavens': 134,\n",
       " 'restless': 135,\n",
       " 'waves': 136,\n",
       " 'skill': 137,\n",
       " 'retires': 138,\n",
       " 'vide': 139,\n",
       " 'returne': 140,\n",
       " 'since': 141,\n",
       " 'while': 142,\n",
       " 'whatsoever': 143,\n",
       " 'tears': 144,\n",
       " 'back': 145,\n",
       " 'ye': 146,\n",
       " 'statements': 147,\n",
       " 'wine': 148,\n",
       " 'raimond': 149,\n",
       " 'your': 150,\n",
       " 'ruin': 151,\n",
       " 'least': 152,\n",
       " 'you': 153,\n",
       " 'take': 154,\n",
       " 'walw': 155,\n",
       " 'picking': 156,\n",
       " 'said': 157,\n",
       " 'royal': 158,\n",
       " 'ballena': 159,\n",
       " 'former': 160,\n",
       " 'mouthed': 161,\n",
       " 'ocean': 162,\n",
       " 'tongue': 163,\n",
       " 'veritable': 164,\n",
       " 'thankless': 165,\n",
       " 'grove': 166,\n",
       " 'whereas': 167,\n",
       " 'this': 168,\n",
       " 'supplied': 169,\n",
       " 'refugees': 170,\n",
       " 'length': 171,\n",
       " 'eight': 172,\n",
       " 'land': 173,\n",
       " 'monstrous': 174,\n",
       " 'late': 175,\n",
       " 'no': 176,\n",
       " 'true': 177,\n",
       " 'could': 178,\n",
       " 'nescio': 179,\n",
       " 'shine': 180,\n",
       " 'strong': 181,\n",
       " 'breast': 182,\n",
       " 'piggledy': 183,\n",
       " 'unsplinterable': 184,\n",
       " 'vessel': 185,\n",
       " 'insomuch': 186,\n",
       " 'indian': 187,\n",
       " 'biggest': 188,\n",
       " 'fly': 189,\n",
       " 'piercing': 190,\n",
       " 'word': 191,\n",
       " 'plainly': 192,\n",
       " 'sebond': 193,\n",
       " 'doubt': 194,\n",
       " 'fish': 195,\n",
       " 'profane': 196,\n",
       " 'modern': 197,\n",
       " 'serpent': 198,\n",
       " 'queen': 199,\n",
       " 'yards': 200,\n",
       " 'his': 201,\n",
       " 'parmacetti': 202,\n",
       " 'vast': 203,\n",
       " 'hosmannus': 204,\n",
       " 'think': 205,\n",
       " 'dinting': 206,\n",
       " 'ancient': 207,\n",
       " 'aloft': 208,\n",
       " 'job': 209,\n",
       " '[': 210,\n",
       " 'glancing': 211,\n",
       " 'that': 212,\n",
       " 'however': 213,\n",
       " 'others': 214,\n",
       " 'he': 215,\n",
       " 'was': 216,\n",
       " 'monsters': 217,\n",
       " 'animal': 218,\n",
       " 'immediately': 219,\n",
       " 'earth': 220,\n",
       " 'as': 221,\n",
       " 'baleine': 222,\n",
       " 'soever': 223,\n",
       " 'roll': 224,\n",
       " 'authentic': 225,\n",
       " 'hand': 226,\n",
       " 'there': 227,\n",
       " 'embellished': 228,\n",
       " 'see': 229,\n",
       " 'genesis': 230,\n",
       " 'ibid': 231,\n",
       " 'horse': 232,\n",
       " 'too': 233,\n",
       " 'crooked': 234,\n",
       " 'sword': 235,\n",
       " 'might': 236,\n",
       " 'not': 237,\n",
       " 'summer': 238,\n",
       " 'raphael': 239,\n",
       " 'bluntly': 240,\n",
       " 'anglo': 241,\n",
       " 'hackluyt': 242,\n",
       " 'unpleasant': 243,\n",
       " 'path': 244,\n",
       " 'us': 245,\n",
       " 'it': 246,\n",
       " 'motion': 247,\n",
       " 'belongest': 248,\n",
       " 'six': 249,\n",
       " 'valuable': 250,\n",
       " 'were': 251,\n",
       " 'wears': 252,\n",
       " 'islands': 253,\n",
       " 'leaving': 254,\n",
       " 'generally': 255,\n",
       " 'whom': 256,\n",
       " 'dragon': 257,\n",
       " 'therein': 258,\n",
       " ';': 259,\n",
       " 'sperma': 260,\n",
       " 'lins': 261,\n",
       " 'let': 262,\n",
       " 'roundness': 263,\n",
       " 'rabelais': 264,\n",
       " 'chaos': 265,\n",
       " 'about': 266,\n",
       " 'gay': 267,\n",
       " 'long': 268,\n",
       " 'glasses': 269,\n",
       " 'goes': 270,\n",
       " 'availle': 271,\n",
       " 'be': 272,\n",
       " 'latin': 273,\n",
       " 'eyes': 274,\n",
       " 'beast': 275,\n",
       " 'againe': 276,\n",
       " 'bodies': 277,\n",
       " 'hamlet': 278,\n",
       " 'worker': 279,\n",
       " 'waller': 280,\n",
       " 'extracts': 281,\n",
       " 'court': 282,\n",
       " 'fegee': 283,\n",
       " 'whales': 284,\n",
       " 'flail': 285,\n",
       " 'wal': 286,\n",
       " 'loved': 287,\n",
       " 'deliver': 288,\n",
       " 'worm': 289,\n",
       " 'sacred': 290,\n",
       " 'herman': 291,\n",
       " 'swallowed': 292,\n",
       " 'together': 293,\n",
       " 'grammars': 294,\n",
       " 'sadness': 295,\n",
       " 'tribe': 296,\n",
       " 'nations': 297,\n",
       " 'entertaining': 298,\n",
       " 'til': 299,\n",
       " 'mortality': 300,\n",
       " 'thee': 301,\n",
       " 'coming': 302,\n",
       " 'apology': 303,\n",
       " 'tail': 304,\n",
       " 'dutch': 305,\n",
       " 'grub': 306,\n",
       " 'fare': 307,\n",
       " '.\"': 308,\n",
       " 'dan': 309,\n",
       " 'nuee': 310,\n",
       " 'peaceful': 311,\n",
       " 'paine': 312,\n",
       " 'sallow': 313,\n",
       " '(': 314,\n",
       " 'catched': 315,\n",
       " 'book': 316,\n",
       " 'devilish': 317,\n",
       " 'commonwealth': 318,\n",
       " 'fifty': 319,\n",
       " 'side': 320,\n",
       " 'extracted': 321,\n",
       " 'ever': 322,\n",
       " 'lost': 323,\n",
       " 'will': 324,\n",
       " '...': 325,\n",
       " 'affording': 326,\n",
       " 'whoel': 327,\n",
       " 'shore': 328,\n",
       " 'catching': 329,\n",
       " 'old': 330,\n",
       " 'wallow': 331,\n",
       " 'lexicons': 332,\n",
       " 'generations': 333,\n",
       " 'altogether': 334,\n",
       " 'towards': 335,\n",
       " 'lord': 336,\n",
       " 'octher': 337,\n",
       " 'sides': 338,\n",
       " ':': 339,\n",
       " 'have': 340,\n",
       " 'justly': 341,\n",
       " 'here': 342,\n",
       " 'give': 343,\n",
       " 'sleeps': 344,\n",
       " 'king': 345,\n",
       " 'had': 346,\n",
       " 'incontinently': 347,\n",
       " 'usher': 348,\n",
       " 'like': 349,\n",
       " 'h': 350,\n",
       " 'on': 351,\n",
       " 'by': 352,\n",
       " 'isaiah': 353,\n",
       " 'devil': 354,\n",
       " 'ponderous': 355,\n",
       " 'perisheth': 356,\n",
       " 'cetus': 357,\n",
       " 'commentator': 358,\n",
       " 'moses': 359,\n",
       " 'moby': 360,\n",
       " 'fancied': 361,\n",
       " 'known': 362,\n",
       " 'other': 363,\n",
       " 'beating': 364,\n",
       " 'visited': 365,\n",
       " 'what': 366,\n",
       " 'every': 367,\n",
       " 'boil': 368,\n",
       " 'appearing': 369,\n",
       " 'trouble': 370,\n",
       " 'before': 371,\n",
       " 'but': 372,\n",
       " 'an': 373,\n",
       " 'thro': 374,\n",
       " 'four': 375,\n",
       " 'gulp': 376,\n",
       " 'would': 377,\n",
       " 'clearing': 378,\n",
       " 'say': 379,\n",
       " 'sunrise': 380,\n",
       " 'promiscuously': 381,\n",
       " 'browne': 382,\n",
       " 'stalls': 383,\n",
       " 'all': 384,\n",
       " 'alfred': 385,\n",
       " 'punish': 386,\n",
       " 'saith': 387,\n",
       " 'strike': 388,\n",
       " 'nick': 389,\n",
       " 'wallen': 390,\n",
       " 'seethe': 391,\n",
       " 'fat': 392,\n",
       " 'out': 393,\n",
       " 'well': 394,\n",
       " 'thing': 395,\n",
       " 'scarcely': 396,\n",
       " 'mote': 397,\n",
       " ')': 398,\n",
       " 'poets': 399,\n",
       " 'lowly': 400,\n",
       " 'so': 401,\n",
       " 'open': 402,\n",
       " 'things': 403,\n",
       " 'pampered': 404,\n",
       " 'years': 405,\n",
       " 'maketh': 406,\n",
       " 'ger': 407,\n",
       " 'mere': 408,\n",
       " 'came': 409,\n",
       " 'version': 410,\n",
       " 'sir': 411,\n",
       " 'plutarch': 412,\n",
       " 'queer': 413,\n",
       " 'cartloads': 414,\n",
       " 'teach': 415,\n",
       " 'tuileries': 416,\n",
       " 'friends': 417,\n",
       " 'anyways': 418,\n",
       " 'nothing': 419,\n",
       " 'which': 420,\n",
       " 'ketos': 421,\n",
       " 'a': 422,\n",
       " 'find': 423,\n",
       " 'librarian': 424,\n",
       " 'dictionary': 425,\n",
       " 'tooke': 426,\n",
       " '1851': 427,\n",
       " 'described': 428,\n",
       " 'lucian': 429,\n",
       " 'boat': 430,\n",
       " 'calm': 431,\n",
       " 'brain': 432,\n",
       " 'richardson': 433,\n",
       " 'whirlpooles': 434,\n",
       " 'killed': 435,\n",
       " 'dart': 436,\n",
       " 'one': 437,\n",
       " 'street': 438,\n",
       " 'country': 439,\n",
       " 'gudgeon': 440,\n",
       " 'ceti': 441,\n",
       " 'inward': 442,\n",
       " 'some': 443,\n",
       " 'jonah': 444,\n",
       " 'flies': 445,\n",
       " 'best': 446,\n",
       " 'life': 447,\n",
       " 'preface': 448,\n",
       " 'secure': 449,\n",
       " 'heart': 450,\n",
       " 'ships': 451,\n",
       " '-': 452,\n",
       " 'made': 453,\n",
       " 'spencer': 454,\n",
       " 'can': 455,\n",
       " 'through': 456,\n",
       " 'greek': 457,\n",
       " 'narrative': 458,\n",
       " 'death': 459,\n",
       " 'noble': 460,\n",
       " '!': 461,\n",
       " 'e': 462,\n",
       " 'stowe': 463,\n",
       " 'hvalt': 464,\n",
       " 'bird': 465,\n",
       " 'reminded': 466,\n",
       " 'gone': 467,\n",
       " 'we': 468,\n",
       " 'certain': 469,\n",
       " 'to': 470,\n",
       " 'mast': 471,\n",
       " 'sore': 472,\n",
       " 'davenant': 473,\n",
       " 'pains': 474,\n",
       " 'appeared': 475,\n",
       " 'if': 476,\n",
       " 'seas': 477,\n",
       " 'dut': 478,\n",
       " 'thought': 479,\n",
       " 'sherry': 480,\n",
       " 'among': 481,\n",
       " 'making': 482,\n",
       " 'pan': 483,\n",
       " 'how': 484,\n",
       " 'eye': 485,\n",
       " 'etymology': 486,\n",
       " 'named': 487,\n",
       " 'seven': 488,\n",
       " 'flags': 489,\n",
       " 'own': 490,\n",
       " 'seen': 491,\n",
       " 'full': 492,\n",
       " 'after': 493,\n",
       " 'sit': 494,\n",
       " 'michael': 495,\n",
       " 'balaene': 496,\n",
       " 'oil': 497,\n",
       " 'arched': 498,\n",
       " 'immense': 499,\n",
       " 'god': 500,\n",
       " 'sovereignest': 501,\n",
       " 'is': 502,\n",
       " 'fishes': 503,\n",
       " 'into': 504,\n",
       " 'touching': 505,\n",
       " 'annals': 506,\n",
       " 'days': 507,\n",
       " 'hopeless': 508,\n",
       " 'coat': 509,\n",
       " 'therefore': 510,\n",
       " 'spanish': 511,\n",
       " 'me': 512,\n",
       " 'wound': 513,\n",
       " 'empty': 514,\n",
       " ']': 515,\n",
       " 'been': 516,\n",
       " 'english': 517,\n",
       " 'work': 518,\n",
       " 'leach': 519,\n",
       " 'mildly': 520,\n",
       " 'these': 521,\n",
       " 'gabriel': 522,\n",
       " 'saxon': 523,\n",
       " 'far': 524,\n",
       " 'swedish': 525,\n",
       " 'hoary': 526,\n",
       " 'ian': 527,\n",
       " 'consumptive': 528,\n",
       " 'they': 529,\n",
       " 'psalms': 530,\n",
       " 'warm': 531,\n",
       " 'signification': 532,\n",
       " 'go': 533,\n",
       " 'within': 534,\n",
       " 'whose': 535,\n",
       " \"'\": 536,\n",
       " 'up': 537,\n",
       " 'talus': 538,\n",
       " 'who': 539,\n",
       " 'view': 540,\n",
       " 'their': 541,\n",
       " 'or': 542,\n",
       " 'subs': 543,\n",
       " 'authors': 544,\n",
       " 'play': 545,\n",
       " 'am': 546,\n",
       " 'against': 547,\n",
       " 'any': 548,\n",
       " 'school': 549,\n",
       " 'gondibert': 550,\n",
       " 'rolling': 551,\n",
       " 'webster': 552,\n",
       " 'vaulted': 553,\n",
       " 'montaigne': 554,\n",
       " 'whale': 555,\n",
       " 'almost': 556,\n",
       " 'sea': 557,\n",
       " 'most': 558,\n",
       " 'two': 559,\n",
       " 'french': 560,\n",
       " 'hval': 561,\n",
       " 'allusions': 562,\n",
       " 'proceeded': 563,\n",
       " 'cometh': 564,\n",
       " 'stone': 565,\n",
       " 'storied': 566,\n",
       " 'threatens': 567,\n",
       " 'must': 568,\n",
       " 'value': 569,\n",
       " 'bones': 570,\n",
       " 'd': 571,\n",
       " 'breedeth': 572,\n",
       " 'faerie': 573,\n",
       " 'art': 574,\n",
       " 'quid': 575,\n",
       " 'wounded': 576,\n",
       " 'i': 577,\n",
       " 'leviathan': 578,\n",
       " 'feel': 579,\n",
       " 'william': 580,\n",
       " 'hwal': 581,\n",
       " 'ork': 582}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "         29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "         43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "         57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
       "         71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
       "         85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
       "         99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "        113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
       "        127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
       "        141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
       "        155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "        183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
       "        197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210,\n",
       "        211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
       "        225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
       "        239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252,\n",
       "        253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
       "        267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
       "        281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294,\n",
       "        295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
       "        309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322,\n",
       "        323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
       "        337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
       "        365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,\n",
       "        379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392,\n",
       "        393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406,\n",
       "        407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420,\n",
       "        421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
       "        435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448,\n",
       "        449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462,\n",
       "        463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476,\n",
       "        477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490,\n",
       "        491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504,\n",
       "        505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518,\n",
       "        519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "        533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546,\n",
       "        547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560,\n",
       "        561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574,\n",
       "        575, 576, 577, 578, 579, 580, 581, 582,   0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 6.09\n",
      "Epoch : 10, mean_loss : 4.35\n",
      "Epoch : 20, mean_loss : 3.47\n",
      "Epoch : 30, mean_loss : 3.31\n",
      "Epoch : 40, mean_loss : 3.26\n",
      "Epoch : 50, mean_loss : 3.24\n",
      "Epoch : 60, mean_loss : 3.22\n",
      "Epoch : 70, mean_loss : 3.21\n",
      "Epoch : 80, mean_loss : 3.21\n",
      "Epoch : 90, mean_loss : 3.20\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        \n",
    "        inputs, targets = zip(*batch)\n",
    "        \n",
    "        inputs = torch.cat(inputs)  ## B X 1\n",
    "        targets = torch.cat(targets)\n",
    "        # tuple to tensor 해줘야함\n",
    "        \n",
    "        vocabs = prepare_sequence(list(vocab), word2index).expand(inputs.size(0), len(vocab))  # B x V\n",
    "                                                                                               # [1,2,3,..,581,0]이게 batch_size만큼 뭉쳐있는게 vocabs                                                                                                \n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = model(inputs, targets, vocabs)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.data)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch,np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(target, vocab): # target과 전체 단어의 유사도\n",
    "    target_V = model.prediction(prepare_word(target, word2index)) # d X 1\n",
    "    similarities = []\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == target: \n",
    "            continue\n",
    "            \n",
    "        vector = model.prediction(prepare_word(list(vocab)[i], word2index)) # 비교대상\n",
    "        cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0] # 타겟과의 cosine_similarity\n",
    "        similarities.append([vocab[i], cosine_sim])\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10]     # sort by similarity       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clearing'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = random.choice(list(vocab))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['storied', 0.6760297417640686],\n",
       " ['leaving', 0.5904781222343445],\n",
       " ['before', 0.5829606652259827],\n",
       " ['seven', 0.5702384114265442],\n",
       " ['have', 0.5668636560440063],\n",
       " [':', 0.5515078902244568],\n",
       " ['out', 0.550243616104126],\n",
       " ['beating', 0.539977490901947],\n",
       " ['hampton', 0.5343126654624939],\n",
       " ['whether', 0.5281474590301514]]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(test,vocab) ## clearing과 유사한단어 상위10개"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
