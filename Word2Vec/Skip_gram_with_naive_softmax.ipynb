{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Skipgram with naiive softmax\n",
    "Regference 김성동님 [DeepNLP-models-Pytorch](https://github.com/DSKSD/DeepNLP-models-Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n",
      "3.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "ByteTensor = torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "        \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch\n",
    "        \n",
    "        ## traindata의 수가 batch보다 작으면 그냥 그 데이터 한번에 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "## \n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return Variable(LongTensor([word2index[word]]) if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]]))\n",
    "## prepare_word : 해당단어의 wordindex LongTensor로 가져옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and Preprocessing\n",
    "\n",
    "#### Load corpus : Gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:100] #  sampling sentences for test\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']'],\n",
       " ['etymology', '.'],\n",
       " ['(',\n",
       "  'supplied',\n",
       "  'by',\n",
       "  'a',\n",
       "  'late',\n",
       "  'consumptive',\n",
       "  'usher',\n",
       "  'to',\n",
       "  'a',\n",
       "  'grammar',\n",
       "  'school',\n",
       "  ')'],\n",
       " ['the',\n",
       "  'pale',\n",
       "  'usher',\n",
       "  '--',\n",
       "  'threadbare',\n",
       "  'in',\n",
       "  'coat',\n",
       "  ',',\n",
       "  'heart',\n",
       "  ',',\n",
       "  'body',\n",
       "  ',',\n",
       "  'and',\n",
       "  'brain',\n",
       "  ';',\n",
       "  'i',\n",
       "  'see',\n",
       "  'him',\n",
       "  'now',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'was',\n",
       "  'ever',\n",
       "  'dusting',\n",
       "  'his',\n",
       "  'old',\n",
       "  'lexicons',\n",
       "  'and',\n",
       "  'grammars',\n",
       "  ',',\n",
       "  'with',\n",
       "  'a',\n",
       "  'queer',\n",
       "  'handkerchief',\n",
       "  ',',\n",
       "  'mockingly',\n",
       "  'embellished',\n",
       "  'with',\n",
       "  'all',\n",
       "  'the',\n",
       "  'gay',\n",
       "  'flags',\n",
       "  'of',\n",
       "  'all',\n",
       "  'the',\n",
       "  'known',\n",
       "  'nations',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  '.']]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Stopwords from unigram distribution's tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(flatten(corpus))\n",
    "border = int(len(word_count) * 0.01)  ## 592개의 단어 있어서 상위 하위 5개 제외할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = word_count.most_common()[:border]+list(reversed(word_count.most_common()))[:border]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords =[s[0] for s in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', 'the', 'of', 'and', 'man', 'artificial', 'civitas', '--(', 'state']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(flatten(corpus)) - set(stopwords))\n",
    "vocab.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592 583\n"
     ]
    }
   ],
   "source": [
    "print(len(set(flatten(corpus))), len(vocab))\n",
    "# 원래 592개에서 10개 불용어 빼고 UNK하나 추가해서 583개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {'<UNK>':0}\n",
    "\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "        \n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "## {0:'<UNK>',1:'however',2:'foul'...}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare train data\n",
    "window data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 3\n",
    "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1)) for c in corpus])\n",
    "# 각각의 corpus에 대해서 window size가 주어지면 첫번째 단어는 왼쪽에 단어가 없어서 Window_size만큼 Dummy를 주는 작업. 뒤에 코딩 편하도록\n",
    "\n",
    "# [('<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by'),\n",
    "#  ('<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman'),\n",
    "#  ('<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville'),\n",
    "#  ('[', 'moby', 'dick', 'by', 'herman', 'melville', '1851'),\n",
    "#  ('moby', 'dick', 'by', 'herman', 'melville', '1851', ']'),\n",
    "#  ('dick', 'by', 'herman', 'melville', '1851', ']', '<DUMMY>'),\n",
    "#  ('by', 'herman', 'melville', '1851', ']', '<DUMMY>', '<DUMMY>'),\n",
    "#  ('herman', 'melville', '1851', ']', '<DUMMY>', '<DUMMY>', '<DUMMY>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1463"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(windows) ## 1463개의 training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by')"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'moby'), ('[', 'dick'), ('[', 'by'), ('moby', '['), ('moby', 'dick'), ('moby', 'by')]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "for window in windows:                                 # Skip gram\n",
    "    for i in range(WINDOW_SIZE * 2 + 1):               # centerword + 2 * WINDOW_SIZE = 7\n",
    "        if i == WINDOW_SIZE or window[i] == '<DUMMY>': # i  = WINDOW_SIZE는 centerword라는 뜻\n",
    "            continue                                   # i번째 단어가 centerword이거나 'DUMMY'이면 넘어가라!\n",
    "        train_data.append((window[WINDOW_SIZE], window[i]))\n",
    "    \n",
    "print(train_data[:WINDOW_SIZE*2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p = []\n",
    "y_p = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[', 'moby')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0] # input이 '[' label 이 'moby'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in train_data:\n",
    "    X_p.append(prepare_word(tr[0], word2index).view(1,-1)) # -1은 미지수 n 차원일부로 늘리는거 (  [] 를 하나 더 씌운다.)\n",
    "    y_p.append(prepare_word(tr[1], word2index).view(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[64]])\n",
      "tensor([[281]])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_p[0])\n",
    "print(y_p[0])\n",
    "print(X_p[0].shape) # 차원하나 더늘려서 (1,1)됨  \n",
    "                    # ex) [[64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7606"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = list(zip(X_p, y_p))\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim)\n",
    "        \n",
    "        self.embedding_v.weight.data.uniform_(-1,1)  # init\n",
    "        self.embedding_u.weight.data.uniform_(0,0)   # init\n",
    "        # self.out = nn.Linear(projection_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, center_words, target_words, outer_words):\n",
    "        # outer_words가 BXV 즉 [1,2,3,4,...582,0]이 Batchsize만큼 있는데\n",
    "        # 이걸 넣으면 VXD의 W' matrix가 Batchsize만큼 있는거\n",
    "        center_embeds = self.embedding_v(center_words) # B X 1 X D  center_word의 인덱스에 해당하는 하나의 행을 가져오는듯\n",
    "        target_embeds = self.embedding_u(target_words) # B X 1 X D\n",
    "        outer_embeds = self.embedding_u(outer_words)   # B X V X D \n",
    "        \n",
    "        scores = target_embeds.bmm(center_embeds.transpose(1,2)).squeeze(2)    # BX1XD * BXDX1 => BX1X1 => BX1   3번째 index가 1이면 없에줌\n",
    "        norm_scores = outer_embeds.bmm(center_embeds.transpose(1,2)).squeeze(2) # BXVXD * BXDX1 => BXVXD * BXDX1=> BXV\n",
    "        \n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/ torch.sum(torch.exp(norm_scores),1).unsqueeze(1))) \n",
    "        # log-softmax 2번째 index에 1을 넣어줌\n",
    "        # torch.sum(torch.exp(num_scores),1) 각 배치별로 Vocab개수만큼의 exp값이 있으면\n",
    "        return nll # negative log likelihood\n",
    "    \n",
    "    def prediction(self,inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        \n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/54069443-6b5afd00-429b-11e9-97bf-ff5e525d93d7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/54069351-ab6db000-429a-11e9-8e99-14764ac89a83.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 30\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "model = Skipgram(len(word2index), EMBEDDING_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = prepare_sequence(list(vocab), word2index).expand(BATCH_SIZE, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1,   2,   3,  ..., 581, 582,   0],\n",
       "        [  1,   2,   3,  ..., 581, 582,   0],\n",
       "        [  1,   2,   3,  ..., 581, 582,   0],\n",
       "        ...,\n",
       "        [  1,   2,   3,  ..., 581, 582,   0],\n",
       "        [  1,   2,   3,  ..., 581, 582,   0],\n",
       "        [  1,   2,   3,  ..., 581, 582,   0]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 6.09\n",
      "Epoch : 10, mean_loss : 4.35\n",
      "Epoch : 20, mean_loss : 3.47\n",
      "Epoch : 30, mean_loss : 3.31\n",
      "Epoch : 40, mean_loss : 3.26\n",
      "Epoch : 50, mean_loss : 3.24\n",
      "Epoch : 60, mean_loss : 3.22\n",
      "Epoch : 70, mean_loss : 3.21\n",
      "Epoch : 80, mean_loss : 3.21\n",
      "Epoch : 90, mean_loss : 3.20\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        \n",
    "        inputs, targets = zip(*batch)\n",
    "        \n",
    "        inputs = torch.cat(inputs)  ## B X 1\n",
    "        targets = torch.cat(targets)\n",
    "        # tuple to tensor 해줘야함\n",
    "        \n",
    "        vocabs = prepare_sequence(list(vocab), word2index).expand(inputs.size(0), len(vocab))  # B x V\n",
    "                                                                                               # [1,2,3,..,581,0]이게 batch_size만큼 뭉쳐있는게 vocabs                                                                                                \n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = model(inputs, targets, vocabs)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.data)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch,np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(target, vocab): # target과 전체 단어의 유사도\n",
    "    target_V = model.prediction(prepare_word(target, word2index)) # d X 1\n",
    "    similarities = []\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == target: \n",
    "            continue\n",
    "            \n",
    "        vector = model.prediction(prepare_word(list(vocab)[i], word2index)) # 비교대상\n",
    "        cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0] # 타겟과의 cosine_similarity\n",
    "        similarities.append([vocab[i], cosine_sim])\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10]     # sort by similarity       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clearing'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = random.choice(list(vocab))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['storied', 0.6760297417640686],\n",
       " ['leaving', 0.5904781222343445],\n",
       " ['before', 0.5829606652259827],\n",
       " ['seven', 0.5702384114265442],\n",
       " ['have', 0.5668636560440063],\n",
       " [':', 0.5515078902244568],\n",
       " ['out', 0.550243616104126],\n",
       " ['beating', 0.539977490901947],\n",
       " ['hampton', 0.5343126654624939],\n",
       " ['whether', 0.5281474590301514]]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(test,vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
